{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQAep6mya0Uv"
   },
   "source": [
    "# Fundamentals of Computer Vision\n",
    "\n",
    "This Jupyter Notebook covers **Project 4** of the course and introduces fundamental concepts of the Feature Descriptors (Normalized Color Histogram, Color Space Changes, SIFT, SURF, and ORB) and Bag of Words in image processing. Each section has questions that must be answered in a Document in PDF format.\n",
    "\n",
    "**Important**\n",
    "\n",
    "Both the Feature Descriptors and Bag of Words tasks (Code + Answers) must be submitted; otherwise, your work will be rejected.\n",
    "\n",
    "\n",
    "## Grading Breakdown: ##\n",
    "- Feature Descriptors: 40 points (Code: 24 pts, Answers: 16 pts).\n",
    "- Bag of Words: 60 points (Code: 36 pts, Answers: 24 pts).\n",
    "\n",
    "To pass Project 4, a minimum of **50 points** is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oorq2s7wGonf"
   },
   "source": [
    "# **Feature Descriptors**\n",
    "\n",
    "## **Normalized Color Histograms and Color Space Transformations**\n",
    "\n",
    "In this exercise, you will implement normalised colour histograms and apply colour space transformations as essential components of feature descriptors. These descriptors capture key characteristics or patterns within an image, enabling effective comparison, matching, and classification. Follow the steps below to complete the implementation:\n",
    "\n",
    "1. **Load and Display the Image:**\n",
    "* Read the input image using OpenCV.\n",
    "* Convert the color format from BGR to RGB for proper visualisation with `matplotlib`.\n",
    "* Display the original image using `matplotlib` to verify the input data.\n",
    "\n",
    "2. **Compute Normalized Histograms for RGB:**\n",
    "* Calculate a normalized histogram to represent the frequency of intensity values.\n",
    "* Ensure the histogram values are scaled to sum to 1 for normalization.\n",
    "* Visualize the histogram to observe and compare intensity distributions.\n",
    "\n",
    "3. **Transform Image to Other Color Spaces:**\n",
    "* Convert the image to different color spaces: HSV, LAB, and YCrCb. Use OpenCV's color conversion functions (`cv2.cvtColor`) to perform the transformations.\n",
    "\n",
    "4. **Compute and Visualize Histograms for Each Color Space:**\n",
    "* Compute normalized histograms.\n",
    "* Visualize the histograms to observe how intensity distributions vary across color spaces.\n",
    "\n",
    "5. **Compare Histograms:**\n",
    "* Compare histograms using a similarity metric, such as correlation (`cv2.compareHis`)\n",
    "\n",
    "6. **Return Results:**\n",
    "* Return all generated histograms and their visualizations.\n",
    "* Provide computed similarity scores for comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSYWEDF0wMl9"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function to compute a normalized histogram\n",
    "def compute_normalized_histogram(image, channel, bins=256, range=(0, 256)):\n",
    "    \"\"\"\n",
    "    Computes a normalized histogram for a specific color channel.\n",
    "    \"\"\"\n",
    "    # Your Code goes Here!\n",
    "    return hist\n",
    "\n",
    "# Function to plot a histogram\n",
    "# Your Code goes Here!\n",
    "\n",
    "# Load and display the original image\n",
    "# Convert BGR to RGB for proper visualization\n",
    "# Your Code goes Here!\n",
    "\n",
    "\n",
    "# Normalized histograms for RGB color space\n",
    "# Your Code goes Here!\n",
    "\n",
    "# Convert the image into other color spaces\n",
    "# Your Code goes Here!\n",
    "\n",
    "# Normalized histograms for other color spaces\n",
    "# Your Code goes Here!\n",
    "\n",
    "# Comparing histograms (e.g., correlation)\n",
    "def compare_histograms(hist1, hist2):\n",
    "    \"\"\"\n",
    "    Compare two histograms using correlation.\n",
    "    \"\"\"\n",
    "    # Your Code goes Here!\n",
    "    return compareHist\n",
    "\n",
    "# Compare histograms\n",
    "# Your Code goes Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLFBRomGY3Rk"
   },
   "source": [
    "**Questions that must be included in your Report**.\n",
    "\n",
    "Answer the questions briefly and directly, including the images obtained from the execution of your code to explain your conclusions. Answers must be connected to the executed code.\n",
    "\n",
    "1. What is the purpose of normalizing histograms, and how does this technique would help in feature extraction for image processing tasks?\n",
    "\n",
    "2. After transforming an image to HSV, LAB, and YCrCb color spaces, how does the interpretation of intensity values in each color space change compared to the RGB color space? Visualize the histograms of each space.\n",
    "\n",
    "3. Apply salt-and-pepper noise to the tree1.jpg image located in the dataset_histogram folder, and save the resulting noisy image as tree1_noisy.jpg in the same folder. Select a color space (e.g., HSV, LAB, or YCrCb) and compute the histograms for all images in the dataset_histogram folder, including tree1.jpg and tree1_noisy.jpg. Finally, **compare the histograms**: evaluate the similarity between the histogram of tree1.jpg and the histograms of all other images in the dataset using a similarity metric such as correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hKrGTS2AgWg"
   },
   "source": [
    "# **SIFT, SURF, and ORB**\n",
    "\n",
    "In this exercise, you will apply features description algorithms: SIFT, SURF, and ORB. These methods enable robust detection of distinctive features within images, which can be used for tasks such as matching, recognition, and tracking. Follow the steps below to complete the implementation:\n",
    "\n",
    "1. **Load and Display Images**\n",
    "\n",
    "* Load diverse images using OpenCV.\n",
    "* Convert them from BGR to RGB for proper visualization.\n",
    "* Display images with Matplotlib to confirm the input data.\n",
    "\n",
    "2. **Implement Feature Extraction**\n",
    "\n",
    "* Use OpenCV's built-in functions to implement the algorithms:\n",
    "  * SIFT: For precise and stable feature detection.\n",
    "  * ORB: For efficient, real-time feature detection.\n",
    "  * SURF (optional): A balance between SIFT's robustness and ORB's speed.\n",
    "\n",
    "3. **Apply Feature Detection**\n",
    "\n",
    "* Run each algorithm on the images.\n",
    "* Detect keypoints and compute descriptors.\n",
    "* Visualize keypoints overlaid on the images.\n",
    "\n",
    "4. **Benchmark Performance**\n",
    "\n",
    "* Count the number of keypoints detected by each algorithm.\n",
    "* Measure runtime to evaluate computational efficiency.\n",
    "\n",
    "5. **Compare and Analyse Results**\n",
    "\n",
    "* Compare the algorithms based on keypoint precision, speed, and robustness to transformations (e.g., scaling or rotation).\n",
    "* Highlight strengths and weaknesses of each.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cVXg6kqz105M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/jupyteruser/.env_lab/lib/python3.12/site-packages (4.12.0.88)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /home/jupyteruser/.env_lab/lib/python3.12/site-packages (from opencv-python) (2.2.6)\n",
      "Downloading opencv_contrib_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (73.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.12.0.88\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python matplotlib\n",
    "# !pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9fhD74Ucm_I_"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 26 (277373957.py, line 30)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef plot_keypoints(image, keypoints, title):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 26\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Function to load images\n",
    "def load_image(path):\n",
    "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "# SIFT implementation\n",
    "def apply_sift(image):\n",
    "    # Your Code goes Here!\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# ORB implementation\n",
    "def apply_orb(image):\n",
    "    # Your Code goes Here!\n",
    "    orb = cv2.ORB_create()\n",
    "    keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# SURF implementation (optional)\n",
    "def apply_surf(image):\n",
    "    # IT IS PATTENTET \n",
    "\n",
    "# Plot keypoints\n",
    "def plot_keypoints(image, keypoints, title):\n",
    "    img_with_keypoints = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(title)\n",
    "    plt.imshow(img_with_keypoints)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Performance analysis\n",
    "def benchmark(feature_extractor, image):\n",
    "    start = time.time()\n",
    "    keypoints, descriptors = feature_extractor(image)\n",
    "    end = time.time()\n",
    "    return len(keypoints), end - start\n",
    "\n",
    "# Main program\n",
    "# Your Code goes Here!\n",
    "img = load_image(\"images/lenna.png\")\n",
    "\n",
    "keypoints, descriptors = apply_orb(img)\n",
    "plot_keypoints(img, keypoints, \"ORB\")\n",
    "\n",
    "keypoints, descriptors = apply_sift(img)\n",
    "plot_keypoints(img, keypoints, \"SIFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dac-7WQMhWYX"
   },
   "source": [
    "**Questions that must be included in your Report**.\n",
    "\n",
    "Answer the questions briefly and directly, including the images obtained from the execution of your code to explain your conclusions. Answers must be connected to the executed code.\n",
    "\n",
    "4. What impact does the computational time of each feature extraction method (SIFT, SURF, ORB) have on processing larger image datasets or real-time applications? Which method is the most efficient in terms of speed?\n",
    "\n",
    "5. How do modifications such as noise, scale, and rotation affect the performance of SIFT, SURF, and ORB in terms of keypoint detection and description?\n",
    "\n",
    "6. How do the detected keypoints appear visually when overlaid on the image for each method (SIFT, SURF, ORB)? Are there any noticeable differences in the locations or distribution of keypoints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0icdb6xw5rN6"
   },
   "source": [
    "# **Bag of Visual Words**\n",
    "\n",
    "In this exercise, you will explore the **Bag of Visual Words (BoVW)** model for image classification. The process involves using **SIFT** (Scale-Invariant Feature Transform) to extract key features from images. These features are then clustered into \"visual words,\" which collectively form a vocabulary for representing the images. Once the visual words are created, machine learning algorithms can be applied to classify images based on these feature representations. By completing this exercise, you will gain hands-on experience in **feature extraction**, **clustering**, and applying **machine learning** for image classification. Follow the steps below to complete the implementation:\n",
    "\n",
    "1. **Dataset Loading:**\n",
    "Load the dataset by traversing the directory structure to obtain image paths and corresponding class labels.\n",
    "\n",
    "2. **Feature Detection and Description:**\n",
    "Extract local features from each image using the SIFT algorithm.\n",
    "\n",
    "3. **Codebook Creation:**\n",
    "Cluster the extracted feature descriptors using k-means clustering to form a visual vocabulary (codebook).\n",
    "\n",
    "4. **Feature Quantization:**\n",
    "Assign each feature descriptor to the nearest visual word in the codebook and generate a histogram representation for each image.\n",
    "\n",
    "5. **Histogram Normalization:**\n",
    "Normalize the histograms with L2 Normalization to ensure comparability across images.\n",
    "\n",
    "6. **Classifier Training:**\n",
    "Train a Support Vector Machine (SVM) using the normalized histograms as input.\n",
    "\n",
    "7. **Classification:**\n",
    "Classify the test images by predicting labels using the trained SVM.\n",
    "\n",
    "8. **Model Evaluation:**\n",
    "Evaluate the model's performance using metrics such as overall accuracy, per-class accuracy, and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ScdsBnXjfg7t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 0 images\n",
      "Test Set: 0 images\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'descriptors_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest Set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Step 1: Extract SIFT Features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m train_descriptors_list = \u001b[43mextract_sift_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_image_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m test_descriptors_list = extract_sift_features(test_image_paths)\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Step 2: Create Codebook using training features\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mextract_sift_features\u001b[39m\u001b[34m(image_paths)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_sift_features\u001b[39m(image_paths):\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Your Code goes Here!\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdescriptors_list\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'descriptors_list' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(paths, classes):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for i, dataset_path in enumerate(paths):\n",
    "        for class_label in classes:\n",
    "            class_dir = os.path.join(dataset_path, class_label)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_file in os.listdir(class_dir):\n",
    "                    if img_file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                        image_paths.append(os.path.join(class_dir, img_file))\n",
    "                        labels.append(classes.index(class_label))\n",
    "    return image_paths, labels\n",
    "\n",
    "# Function to extract SIFT features\n",
    "def extract_sift_features(image_paths):\n",
    "    # Your Code goes Here!\n",
    "    return descriptors_list\n",
    "\n",
    "# Function to create the codebook\n",
    "def create_codebook(descriptors_list, num_clusters=50):\n",
    "    # Your Code goes Here!\n",
    "    return kmeans\n",
    "\n",
    "# Function to quantize features\n",
    "def quantize_features(descriptors_list, kmeans):\n",
    "    # Your Code goes Here!\n",
    "    return np.array(histograms)\n",
    "\n",
    "# Function to normalize histograms\n",
    "def normalize_histograms(histograms):\n",
    "    return histograms / np.linalg.norm(histograms, axis=1, keepdims=True)\n",
    "\n",
    "# Function to train the SVM\n",
    "def train_svm(histograms, labels):\n",
    "    # Your Code goes Here!\n",
    "    return svm, scaler\n",
    "\n",
    "# Function to classify images\n",
    "def classify_images(histograms, svm, scaler):\n",
    "    # Your Code goes Here!\n",
    "    return svm.predict(histograms_scaled)\n",
    "\n",
    "# Function to evaluate per-class performance\n",
    "def evaluate_per_class(true_labels, predictions, classes):\n",
    "    # Your Code goes Here!\n",
    "    print(report)\n",
    "    return\n",
    "\n",
    "# Dataset configuration\n",
    "CLASSES = ['aquarium', 'desert'] # Only use these two classes.\n",
    "PATHS = ('/dataset_bovw/train/',\n",
    "         '/dataset_bovw/test/')\n",
    "\n",
    "# Load dataset\n",
    "train_image_paths, train_labels = load_dataset([PATHS[0]], CLASSES)\n",
    "test_image_paths, test_labels = load_dataset([PATHS[1]], CLASSES)\n",
    "\n",
    "# Print dataset summary\n",
    "print(f\"Training Set: {len(train_image_paths)} images\")\n",
    "print(f\"Test Set: {len(test_image_paths)} images\")\n",
    "\n",
    "# Step 1: Extract SIFT Features\n",
    "train_descriptors_list = extract_sift_features(train_image_paths)\n",
    "test_descriptors_list = extract_sift_features(test_image_paths)\n",
    "\n",
    "# Step 2: Create Codebook using training features\n",
    "num_clusters = 10  # Adjust this based on your dataset\n",
    "kmeans = create_codebook(train_descriptors_list, num_clusters)\n",
    "\n",
    "# Step 3: Quantize Features\n",
    "train_histograms = quantize_features(train_descriptors_list, kmeans)\n",
    "test_histograms = quantize_features(test_descriptors_list, kmeans)\n",
    "\n",
    "# Step 4: Normalize Histograms\n",
    "normalized_train_histograms = normalize_histograms(train_histograms)\n",
    "normalized_test_histograms = normalize_histograms(test_histograms)\n",
    "\n",
    "# Step 5: Train Classifier\n",
    "svm, scaler = train_svm(normalized_train_histograms, train_labels)\n",
    "\n",
    "# Step 6: Classify Test Images\n",
    "test_predictions = classify_images(normalized_test_histograms, svm, scaler)\n",
    "\n",
    "# Step 7: Evaluate Model Per-class\n",
    "evaluate_per_class(test_labels, test_predictions, CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QSJNqfc6X4K"
   },
   "source": [
    "**Questions that must be included in your Report**.\n",
    "\n",
    "Answer the questions briefly and directly, including the images obtained from the execution of your code to explain your conclusions. Answers must be connected to the executed code.\n",
    "\n",
    "7. What role does the SIFT algorithm play in the BoVW model, and how is it used for feature extraction?\n",
    "\n",
    "8. Explain the process of creating the vocabulary (codebook) in the BoVW model. Why is clustering important, and how does K-Means contribute to this process?\n",
    "\n",
    "9. How do you convert the feature descriptors of images into histograms of visual words? What is the importance of the histogram in the BoVW framework?\n",
    "\n",
    "10. Why is normalization of the histograms necessary before feeding them into the classifier? What method is used for normalization in this exercise?\n",
    "\n",
    "11. Evaluate the model on the test set by calculating precision and recall for each class, and generating a confusion matrix to analyse its performance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
